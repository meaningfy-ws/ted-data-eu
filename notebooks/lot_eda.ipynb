{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# This python notebook is used to perform Exploratory Data Analysis (EDA) on the Lot table from TED data.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Select only the Lot table from the postgres database"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from ted_data_eu.services.etl_pipelines.postgres_etl_pipeline import POSTGRES_URL, SQLALCHEMY_ISOLATION_LEVEL\n",
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "\n",
    "CSV_FILE_PATH = '/home/mihai/work/meaningfy/ted-data-eu/notebooks/exported_data.csv'\n",
    "DB_TABLE = \"Lot\"\n",
    "DB_QUERY = f\"\"\"\n",
    "            SELECT * FROM public.\"{DB_TABLE}\"\n",
    "            \"\"\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Connect to the database and download the information in a csv file"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "sql_engine = sqlalchemy.create_engine(POSTGRES_URL, echo=False, isolation_level=SQLALCHEMY_ISOLATION_LEVEL)\n",
    "with sql_engine.connect() as sql_connection:\n",
    "    df = pd.read_sql(DB_QUERY, sql_connection)\n",
    "df.to_csv (CSV_FILE_PATH, index = False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the file and view the basic information"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_table = pd.read_csv(CSV_FILE_PATH)\n",
    "print(data_table.info())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For each field in the table, represent the distribution by the number of characters (bar chart)\n",
    "and calculate std, average, percentile1, percentile99, min, max, median, iqr, z-score."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "deleted_columns = ['LotId', 'ProcedureId', 'isUsingEUFunds']\n",
    "adjusted_df = data_table.drop(columns=deleted_columns)\n",
    "\n",
    "\n",
    "def add_sphere_trace(fig, x, color, label):\n",
    "    \"\"\"\n",
    "    Creates spheres of indicators in the histogram\n",
    "    :param fig: the figure itself\n",
    "    :param x: calculated indicators\n",
    "    :param color: color\n",
    "    :param label: the name of the sphere representing the indicator\n",
    "    :return: the ID of the added view\n",
    "    \"\"\"\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[x],\n",
    "        y=[0],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=12,\n",
    "            symbol='circle',\n",
    "            color=color,\n",
    "            line=dict(color='black', width=1),\n",
    "            opacity=0.7\n",
    "        ),\n",
    "        name=label\n",
    "    ))\n",
    "\n",
    "\n",
    "def generate_histogram(data, column, nbinsx):\n",
    "    \"\"\"\n",
    "    Generates the histogram of each field from the table\n",
    "    :param data: the length of each field\n",
    "    :param column: name of the column\n",
    "    :param nbinsx: number of bins\n",
    "    :return: the figure with the histogram result\n",
    "    \"\"\"\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    std = data.std()\n",
    "    average = data.mean()\n",
    "    percentile1 = data.quantile(0.01)\n",
    "    percentile99 = data.quantile(0.99)\n",
    "    min = data.min()\n",
    "    max = data.max()\n",
    "    median = data.median()\n",
    "    z_score = (data - average) / std\n",
    "    iqr = data.quantile(0.75) - data.quantile(0.25)\n",
    "\n",
    "    fig.add_trace(go.Histogram(x=data, nbinsx=nbinsx))\n",
    "    std_line_color = 'white'\n",
    "    std_line_width = 6\n",
    "\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=0,\n",
    "        y0=std,\n",
    "        x1=nbinsx,\n",
    "        y1=std,\n",
    "        line=dict(color=std_line_color, width=std_line_width, dash=\"dash\"),\n",
    "        layer='below'\n",
    "    )\n",
    "\n",
    "    add_sphere_trace(fig, average, 'red', 'Average')\n",
    "    add_sphere_trace(fig, std, 'white', 'STD')\n",
    "    add_sphere_trace(fig, percentile1, 'blue', 'Percentile 1')\n",
    "    add_sphere_trace(fig, percentile99, 'blue', 'Percentile 99')\n",
    "    add_sphere_trace(fig, min, 'green', 'Min')\n",
    "    add_sphere_trace(fig, max, 'green', 'Max')\n",
    "    add_sphere_trace(fig, median, 'orange', 'Median')\n",
    "    add_sphere_trace(fig, z_score, 'purple', 'Z-Score')\n",
    "    add_sphere_trace(fig, iqr, 'yellow', 'IQR')\n",
    "\n",
    "    title = f'<b>Distribution of the length of the string for {column}</b><br>'\n",
    "    title += f'Std: {std:.2f}, Average: {average:.2f}, Percentile 1: {percentile1:.2f}, Percentile 99: {percentile99:.2f}<br>'\n",
    "    title += f'Min: {min:.2f}, Max: {max:.2f}, Median: {median:.2f}, IQR: {iqr:.2f}, Z_Score: {z_score.values} <br>'\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title='The length of the string',\n",
    "        yaxis_title='Number of records',\n",
    "        title_font=dict(size=14)\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "for column in adjusted_df.columns:\n",
    "    if adjusted_df[column].dtype == 'object':\n",
    "        adjusted_df['number_of_characters'] = adjusted_df[column].str.len()\n",
    "        generate_histogram(adjusted_df['number_of_characters'], column, nbinsx=100)\n",
    "    elif adjusted_df[column].dtype == 'int64':\n",
    "        generate_histogram(adjusted_df[column], column, nbinsx=100)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calculate KPIs (data completeness, consistency, uniqueness)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_data_consistency(df, column):\n",
    "    \"\"\"\n",
    "    Calculates the data consistency for a specific column in the DataFrame\n",
    "    :param df: dataFrame containing the data\n",
    "    :param column: name of the column for which data consistency is calculated\n",
    "    :return:  data consistency percentage as a floating-point number\n",
    "    \"\"\"\n",
    "    number_of_records = len(df[column])\n",
    "    adjusted_df[f'length {column}'] = adjusted_df[column].str.len()\n",
    "    percentile_1 = adjusted_df[f'length {column}'].quantile(0.01)\n",
    "    percentile_99 = adjusted_df[f'length {column}'].quantile(0.99)\n",
    "    adjusted_df[f'threshold {column}'] = adjusted_df[f'length {column}'].between(percentile_1, percentile_99)\n",
    "    number_of_items = adjusted_df[f'threshold {column}'].sum()\n",
    "    data_consistency = (number_of_items / number_of_records) * 100\n",
    "    return data_consistency\n",
    "\n",
    "def calculate_data_completeness(df, column):\n",
    "    \"\"\"\n",
    "    Calculates the data completeness for a specific column in the DataFrame\n",
    "    :param df: dataFrame containing the data\n",
    "    :param column: name of the column for which data completeness is calculated\n",
    "    :return: data completeness percentage as a floating-point number\n",
    "    \"\"\"\n",
    "    number_of_records = len(df[column])\n",
    "    number_of_records_not_null = df[column].notnull().sum()\n",
    "    data_completeness = (number_of_records_not_null / number_of_records) * 100\n",
    "    return data_completeness\n",
    "\n",
    "def calculate_data_uniqueness(df, column):\n",
    "    \"\"\"\n",
    "    Calculates the data uniqueness for a specific column in the DataFrame\n",
    "    :param df: dataFrame containing the data\n",
    "    :param column: name of the column for which data uniqueness is calculated\n",
    "    :return: data uniqueness percentage as a floating-point number\n",
    "    \"\"\"\n",
    "    number_of_records = len(df[column])\n",
    "    number_of_unique_values = df[column].nunique()\n",
    "    data_uniqueness = (number_of_unique_values / number_of_records) * 100\n",
    "    return data_uniqueness\n",
    "\n",
    "columns_to_calculate = [\"LotTitle\", \"LotEstimatedValueEUR\", \"LotDescription\"]\n",
    "for column in columns_to_calculate:\n",
    "    if column == \"LotEstimatedValueEUR\":\n",
    "        total_values = len(adjusted_df['LotEstimatedValueEUR'])\n",
    "        num_zeros = (adjusted_df['LotEstimatedValueEUR'] == 0).sum()\n",
    "        num_non_zeros = (adjusted_df['LotEstimatedValueEUR'] != 0).sum()\n",
    "        percentage_zeros = (num_zeros / total_values) * 100\n",
    "        percentage_non_zeros = (num_non_zeros / total_values) * 100\n",
    "        print(f\"The KPI data completeness in the LotEstimatedValueEUR field is {percentage_non_zeros:.2f}% and the percentage with zero values is {percentage_zeros:.2f}%.\")\n",
    "\n",
    "    elif column == \"LotDescription\":\n",
    "        data_completeness = calculate_data_completeness(adjusted_df, column)\n",
    "        data_consistency = calculate_data_consistency(adjusted_df, column)\n",
    "        print(f\"The KPI data completeness in the {column} field is {data_completeness:.2f}%.\")\n",
    "        print(f\"The KPI data consistency in the {column} field is {data_consistency:.2f}%.\")\n",
    "\n",
    "    else:\n",
    "        data_consistency = calculate_data_consistency(adjusted_df, column)\n",
    "        data_completeness = calculate_data_completeness(adjusted_df, column)\n",
    "        data_uniqueness = calculate_data_uniqueness(adjusted_df, column)\n",
    "        print(f\"The KPI data consistency in the {column} field is {data_consistency:.2f}%.\")\n",
    "        print(f\"The KPI data completeness in the {column} field is {data_completeness:.2f}%.\")\n",
    "        print(f\"The KPI data uniqueness in the {column} field is {data_uniqueness:.2f}%.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}